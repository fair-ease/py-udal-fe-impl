import io
from pathlib import Path
import zipfile
from SPARQLWrapper import SPARQLWrapper, JSON
import requests
import os
import xarray as xr
from typing import List, Dict, Union
import tempfile
import intake

from ..config import Config

from ..broker import Broker
from ..namedqueries import NamedQueryInfo, QueryName, QUERY_NAMES, QUERY_REGISTRY
from ..result import Result

iddasBrokerQueryName: List[QueryName] = [
    'urn:fairease.eu:argo:data',
]

iddasBrokerQueries: dict[QueryName, NamedQueryInfo] = \
    { k: v for k, v in QUERY_REGISTRY.items() if k in iddasBrokerQueryName }


class IDDASBroker(Broker):

    _queryNames: List[QueryName] = iddasBrokerQueryName
    
    _config: Config


    _queries: dict[QueryName, NamedQueryInfo] = iddasBrokerQueries

    @property
    def queryNames(self) -> List[str]:
        return list(IDDASBroker._queryNames)

    @property
    def queries(self) -> List[NamedQueryInfo]:
        return list(IDDASBroker._queries.values())

    def __init__(self, config: Config):
        self.dict_params = {
                'temperature': 'TEMP',
                'salinity': 'PSAL',
                'pressure': 'PRES'
            }
        self._config = config
        if not self._config or not self._config.blue_cloud_token:
            raise ValueError('Please provide a token')
        self.token = self._config.blue_cloud_token

        self.catalog = None
        self.base_url = 'https://data.blue-cloud.org/api'
        self.sparql_url = 'https://fair-ease-iddas.maris.nl/sparql/query'
        self.headers = {'Authorization': f'Bearer {self.token}', 'Content-Type': 'application/json'}
    
    def _extract_query_param(self, query: str, param: str) -> str:
        """Extracts the value of a query parameter from a query string."""
        try:
            return query.split(f"{param}=")[1].split("&")[0]
        except IndexError:
            raise ValueError(f"Query parameter '{param}' not found in the query string.")
        
    def _build_sparql_filter(self, params: Dict[str, Union[str, float, int]]) -> str:
        """Builds SPARQL filter string from parameters."""
        sparql_filter = []

        # Validate input parameters
        if not any(k in params for k in ('parameter', 'startTime', 'endTime')):
            raise ValueError('Please provide at least one of the following parameters: parameter, startTime, endTime')
     
        if 'parameter' in params:
            param_value = params['parameter']
            
            if isinstance(param_value, str):
                if param_value not in self.dict_params:
                    raise ValueError(f"Parameter '{param_value}' not supported. Please select one of the following parameters: {', '.join(self.dict_params.keys())}")
            
            elif isinstance(param_value, list):
                unsupported_params = [p for p in param_value if p not in self.dict_params]
                if unsupported_params:
                    raise ValueError(f"Parameters '{', '.join(unsupported_params)}' not supported. Please select one of the following parameters: {', '.join(self.dict_params.keys())}")
            
            else:
                raise TypeError(f"Expected parameter to be a string or a list of strings, but got {type(param_value).__name__}.")

        if 'startTime' in params:
            sparql_filter.append(f"FILTER(BOUND(?startDate) && ?startDate >= '{params['startTime']}'^^xsd:date) .")
        if 'endTime' in params:
            sparql_filter.append(f"FILTER(BOUND(?endDate) && ?endDate <= '{params['endTime']}'^^xsd:date) .")
        if 'latitude' in params and 'longitude' in params:
            # Use a bounding box of 10 degrees around the provided latitude and longitude
            # Otherwise SPARQL would search for the exact point
            min_latitude = params['latitude'] - 10
            max_latitude = params['latitude'] + 10
            min_longitude = params['longitude'] - 10
            max_longitude = params['longitude'] + 10

            bbox_wkt = f"POLYGON(({min_longitude} {min_latitude}, {min_longitude} {max_latitude}, {max_longitude} {max_latitude}, {max_longitude} {min_latitude}, {min_longitude} {min_latitude}))"

            sparql_filter.append(f"""
                FILTER(BOUND(?bbox) && geof:sfWithin(?bbox, '{bbox_wkt}'^^geo:wktLiteral))
            """)


        elif 'latitude' in params or 'longitude' in params:
            raise ValueError("Both latitude and longitude must be provided.")
        if 'bounding_box' in params:
            if not isinstance(params['bounding_box'], dict):
                raise ValueError("Bounding box must be a dictionary with keys 'north', 'east', 'south', 'west'.")
            if not all(k in params['bounding_box'] for k in ['north', 'east', 'south', 'west']):
                raise ValueError("Bounding box must be a dictionary with keys 'north', 'east', 'south', 'west'.")
            north = params['bounding_box'].get('north')
            east = params['bounding_box'].get('east')
            south = params['bounding_box'].get('south')
            west = params['bounding_box'].get('west')
            sparql_filter.append(
                f"FILTER(BOUND(?bbox) && geof:sfWithin(?bbox, 'POLYGON (({north} {east}, "
                f"{north} {west}, {south} {west}, "
                f"{south} {east}, {north} {east}))'^^geo:wktLiteral)) ."
            )

        return " ".join(sparql_filter)

    def _get_list_distribution(self, results: dict) -> List[str]:
        """Extracts the list of distributions from the SPARQL query results."""
        if not results['results']['bindings']:
            raise Exception('No data has been found for your query, please update your input fields and try again.')

        for result in results['results']['bindings']:
            if 'netcdf' not in result['mediaType']['value']:
                raise Exception(f"Media type '{result['mediaType']['value']}' is not supported. Please select a NetCDF media type.")

        list_distribution = [
            result['distribution']['value'].replace("#distribution", "")
            for result in results['results']['bindings']
        ]

        if not list_distribution:
            raise Exception('No data has been found for your query, please update your input fields and try again.')

        return list_distribution

    def _prepare_file_names(self, file_name: str, list_distribution: List[str]) -> List[str]:
        """Prepares platform cycle and file names from distributions."""
        file_name = str(file_name)
        if self.catalog == "argo":
            list_plataform_cycle = [
                f"platform-{self._extract_query_param(dist, 'platform')}_cycle-{self._extract_query_param(dist, 'cycle')}"
                for dist in list_distribution
            ]
            list_files = [
                f"{file_name.split('.nc')[0]}_{plataform_cycle}.nc"
                for plataform_cycle in list_plataform_cycle
            ]
            return list_files

        raise ValueError("Catalog not supported.")

    def _remove_existing_files(self, list_files: List[str], list_distribution: List[str]):
        """Removes existing files from the distribution list."""
        for file in list_files:
            if os.path.exists(file):
                for distribution in list_distribution:
                    if self.catalog == 'argo':
                        platform_cycle = (
                            f'platform-{self._extract_query_param(distribution, "platform")}_cycle-'
                            f'{self._extract_query_param(distribution, "cycle")}'
                        )
                        if platform_cycle in file:
                            list_distribution.remove(distribution)
                    else : 
                        raise ValueError("Catalog not supported.")
                    
        return list_distribution

    def _create_folder_name(self, params: Dict[str, Union[str, float, int]]) -> str:
        """Creates a folder name based on parameters."""
        folder_name_filter = ""
        if 'parameter' in params:
            parameter = params['parameter']
            if isinstance(parameter, list):
                folder_name_filter += "_".join(parameter)
            else:
                folder_name_filter += str(parameter)
        if 'startTime' in params:
            folder_name_filter += f"_{params['startTime']}"
        if 'endTime' in params:
            folder_name_filter += f"_{params['endTime']}"
        if 'latitude' in params and 'longitude' in params:
            folder_name_filter += f"_{params['latitude']}_{params['longitude']}"
        if 'bounding_box' in params:
            folder_name_filter += f"_{params['bounding_box']['north']}_{params['bounding_box']['east']}_{params['bounding_box']['south']}_{params['bounding_box']['west']}"

        return folder_name_filter.replace(" ", "_").replace(":", "_").replace("-", "_").replace(",", "_")

    def _execute_argo(self, params: dict):
        """Executes the ARGO data retrieval process."""
        self.catalog = "argo"
        sparql_filter = self._build_sparql_filter(params)
        folder_name_filter = self._create_folder_name(params)
        file_name = f"iddas_{self.catalog}.nc"
        query = f"""
        PREFIX dcat: <http://www.w3.org/ns/dcat#>
        PREFIX dc: <http://purl.org/dc/terms/>
        PREFIX prov: <http://www.w3.org/ns/prov#>
        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
        PREFIX geof: <http://www.opengis.net/def/function/geosparql/>
        PREFIX geo: <http://www.opengis.net/ont/geosparql#>
        PREFIX schema: <https://schema.org/>
        SELECT DISTINCT ?distribution ?mediaType ?downloadURL WHERE {{
            ?dataset a dcat:Dataset ;
                dc:title ?_title ;
                dc:description ?description .
            OPTIONAL {{
                ?dataset dc:temporal [
                    a dc:PeriodOfTime ;
                    dcat:startDate ?startDate ;
                    dcat:endDate ?endDate
                ] .
            }}
            OPTIONAL {{
                ?dataset dc:spatial [
                    a dc:Location ;
                    dcat:bbox ?bbox
                ] .
            }}
            OPTIONAL {{
                ?dataset schema:variableMeasured [
                    a schema:PropertyValue ;
                    schema:name ?parameterName
                ] .
            }}
            OPTIONAL {{
                ?dataset prov:used ?used .
            }}
            OPTIONAL {{
                ?catalog a dcat:Catalog ;
                dcat:dataset ?dataset .
            }}

            ?dataset dcat:distribution ?distribution .
            ?distribution dcat:downloadURL ?downloadURL .
            ?distribution dcat:mediaType ?mediaType .

            FILTER (BOUND(?catalog) && STRSTARTS(STR(?catalog), 'https://data.blue-cloud.org/search/dcat/argo') ) . 
            {sparql_filter}
        }} GROUP BY ?dataset ?distribution ?mediaType ?downloadURL"""
        
        sparql = SPARQLWrapper(self.sparql_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(JSON)
        results = sparql.query().convert()

        def download_and_process_files(dir: Path, file_name: str, list_distribution: List[str], results: dict):
            list_distribution = self._get_list_distribution(results)

            download_urls = []

            for result in results['results']['bindings']:
                if result['downloadURL']['value']:
                    download_urls.append(result['downloadURL']['value'])

            for i, download_url in enumerate(download_urls):
                list_plataform_cycle = [
                    f"platform-{self._extract_query_param(dist, 'platform')}_cycle-{self._extract_query_param(dist, 'cycle')}"
                    for dist in list_distribution
                ]
                file_name_temp = f"{file_name.split('.nc')[0]}_{list_plataform_cycle[i]}.nc"
                
                header = {'Authorization': f'Bearer {self.token}', 'Content-Type': 'application/zip'}
                response = requests.get(download_url, headers=header)
                with zipfile.ZipFile(io.BytesIO(response.content)) as z:
                    for file in z.namelist():
                        if file.endswith('_prof.nc'):
                            z.extract(file, dir)
                            os.rename(dir.joinpath(file), dir.joinpath(file_name_temp))

        def do_processing(dataset: xr.Dataset, params: dict):
            dataset.close()
            try:
                list_data_vars = ['JULD', 'LATITUDE', 'LONGITUDE']

                parameter = params['parameter']
                if isinstance(parameter, str):
                    list_data_vars.append(self.dict_params[parameter])
                elif isinstance(parameter, list):
                    for param in parameter:
                        list_data_vars.append(self.dict_params[param])

                dataset = dataset[list_data_vars]

                return dataset
            except KeyError:
                return None
            except Exception as e:
                raise Exception(f'Error: {e}')

        def process_and_return_datasets(dir: Path, params: dict):
            ds = []
            for file in dir.iterdir():
                dataset = do_processing(xr.open_dataset(file), params)
                if dataset is not None:
                    ds.append(dataset)
    
            return ds
        if self._config.cache_dir is None:
            with tempfile.TemporaryDirectory(prefix='fairease-udal-') as temp_dir:
                dir = Path(temp_dir).joinpath(folder_name_filter)
                try:
                    os.mkdir(dir)
                except FileExistsError:
                    pass

                list_distribution = self._get_list_distribution(results)

                download_and_process_files(dir, file_name, list_distribution, results)
                return process_and_return_datasets(dir, params)

        else:
            dir = Path(self._config.cache_dir).joinpath(folder_name_filter)
            try:
                os.mkdir(dir)
            except FileExistsError:
                pass

            list_distribution = self._get_list_distribution(results)
            list_files = self._prepare_file_names(dir.joinpath(file_name), list_distribution)
            list_distribution = self._remove_existing_files(list_files, list_distribution)

            if list_distribution:
                download_and_process_files(dir, file_name, list_distribution, results)

            return process_and_return_datasets(dir, params)

    def _execute_openeo(self, params: dict):
        """Executes the openeo data retrieval process."""
        self.catalog = "openeo"
        sparql_filter = self._build_sparql_filter(params)
        query = f"""
        PREFIX dcat: <http://www.w3.org/ns/dcat#>
        PREFIX dc: <http://purl.org/dc/terms/>
        PREFIX prov: <http://www.w3.org/ns/prov#>
        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
        PREFIX geof: <http://www.opengis.net/def/function/geosparql/>
        PREFIX geo: <http://www.opengis.net/ont/geosparql#>
        PREFIX schema: <https://schema.org/>
        SELECT DISTINCT ?distribution ?accessURL WHERE {{
            ?dataset a dcat:Dataset ;
                dc:title ?_title ;
                dc:description ?description .
            OPTIONAL {{
                ?dataset dc:temporal [
                    a dc:PeriodOfTime ;
                    dcat:startDate ?startDate ;
                    dcat:endDate ?endDate
                ] .
            }}
            OPTIONAL {{
                ?dataset dc:spatial [
                    a dc:Location ;
                    dcat:bbox ?bbox
                ] .
            }}
            OPTIONAL {{
                ?dataset schema:variableMeasured [
                    a schema:PropertyValue ;
                    schema:name ?parameterName
                ] .
            }}
            OPTIONAL {{
                ?dataset prov:used ?used .
            }}
            OPTIONAL {{
                ?catalog a dcat:Catalog ;
                dcat:dataset ?dataset .
            }}
            OPTIONAL {{
                ?dataset dcat:distribution ?distribution .
            }}
            OPTIONAL {{
  	            ?distribution dcat:accessURL ?accessURL .
            }}

            
            FILTER (BOUND(?catalog) && STRSTARTS(STR(?catalog), 'https://dataset.geodab.eu/source/marineID') ) . 
            FILTER(BOUND(?accessURL) && STRENDS(STR(?accessURL), 'stac.json') ) .
            {sparql_filter}
        }} GROUP BY ?dataset ?distribution ?accessURL LIMIT 10"""
        sparql = SPARQLWrapper(self.sparql_url)
        sparql.setQuery(query)
        sparql.setReturnFormat(JSON)
        results = sparql.query().convert()
        if (not results or not results['results'] or not results['results']['bindings']):
            raise Exception('No data has been found for your query, please update your input fields and try again.')
        
        accessURLs = [result['accessURL']['value'] for result in results['results']['bindings']]
        
        catalogs = []

        for accessURL in accessURLs:
            response = requests.get(accessURL)

            if response.status_code != 200:
                print(f"Error: {response.status_code} - {accessURL}")
                continue 

            stac_obj = self.open_stac_object(response.json())  
            if isinstance(stac_obj, pystac.Collection):
                for item in stac_obj.get_all_items():
                    ds = intake.open_stac_item(item)
                    catalogs.append(ds)
            elif isinstance(stac_obj, pystac.Item):
                ds = intake.open_stac_item(stac_obj)
                catalogs.append(ds)
            else:
                raise ValueError(f"Unsupported STAC object type: {type(stac_obj)}")

        
        datasets= []

        for catalog in catalogs:
            for name in catalog:
                try:
                    ds = catalog[name].to_dask()
                    datasets.append(ds)
                except Exception as e:
                    pass

        ds_xarray = []
        for ds in datasets:
            if type(ds) == str:
                print(f"Error: {ds}")
                continue
            ds_xarray.append(ds)

        return ds_xarray
    

    def execute(self, urn: QueryName, params: dict|None = None) -> Result:
        query = IDDASBroker._queries[urn]
        queryParams = params or {}

        if urn == 'urn:fairease.eu:argo:data':
            return Result(query, self._execute_argo(queryParams))
        else:
            if urn in QUERY_NAMES:
                raise Exception(f'unsupported query name "{urn}"')
            else :
                raise Exception(f'unknown query name "{urn}"')

    